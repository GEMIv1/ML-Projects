{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b6d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Using fullbatch size (The whole dataset in one batch)\n",
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self,X,y,n_hidden_neurons,output_act_fn = 'linear',error_fn = 'mse'):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.input_neurons = X.shape[1] #number of cols\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.output_act_fn = output_act_fn\n",
    "        self.error_fn = error_fn\n",
    "\n",
    "        # Initialize weights  and biases with random values\n",
    "        np.random.seed(42)\n",
    "        self.input_hidden_weights = np.random.randn(self.input_neurons, self.n_hidden_neurons)\n",
    "        self.n_hidden_biases = np.zeros(self.n_hidden_neurons)\n",
    "        self.hidden_output_weights = np.random.randn(n_hidden_neurons,1)\n",
    "        self.output_bias = np.zeros(1)\n",
    "\n",
    "    def activation_fn(self,x,act_fn): # x->refers to the input values\n",
    "        if act_fn == 'sigmoid':\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif act_fn == 'relu':\n",
    "            return np.maximum(0,x)\n",
    "        elif act_fn == 'linear':\n",
    "            return x\n",
    "        else:\n",
    "            return Exception(\"Unknown activation function\")\n",
    "    \n",
    "    def activation_derv(self,x,act_fn):# x->refers to the input values\n",
    "        '''\n",
    "        derv(Sigmoid(x)) = sigmoid(x) * 1 - sigmoid(x)\n",
    "        derv(relu(x)) = 1  if x > 0, 0  if x â‰¤ 0\n",
    "\n",
    "        derv with respect to (x)\n",
    "\n",
    "        '''\n",
    "\n",
    "        if act_fn == 'sigmoid':\n",
    "            s = self.activation_fn(x, act_fn)\n",
    "            return s * (1-s)\n",
    "        if act_fn == 'relu':\n",
    "            return np.where(x>0,1,0)\n",
    "        elif act_fn == 'linear':\n",
    "            return 1\n",
    "\n",
    "    def forward_pass(self,X): # Returns the first prediction without any optimization\n",
    "\n",
    "        # Hidden layer\n",
    "        self.hidden = self.activation_fn((X @ self.input_hidden_weights) + self.n_hidden_biases, 'relu')\n",
    "        # Output layer\n",
    "        self.output = self.activation_fn((self.hidden @ self.hidden_output_weights) + self.output_bias, self.output_act_fn)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def error_estimation(self,y_true,y_pred):\n",
    "\n",
    "        if self.error_fn=='mse':\n",
    "            return np.mean((y_true - y_pred)**2)\n",
    "        \n",
    "        elif self.error_fn=='log loss':\n",
    "            eps = 1e-15 # to prevent log(0) -> inf\n",
    "            return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n",
    "    \n",
    "    def error_estimation_derv(self, y_pred, y_true): # derv with respect to y_pred\n",
    "        \n",
    "        if self.error_fn=='mse':\n",
    "            return -2 * (y_true - y_pred)/y_true.size\n",
    "        \n",
    "        elif self.error_fn == 'log loss':\n",
    "            eps = 1e-15 # to avoid division by 0\n",
    "            return (y_pred - y_true + eps) / ((y_pred+eps) * (1 - y_pred + eps))\n",
    "\n",
    "    def backpropagation(self,X,y_true,y_pred,learning_rate):\n",
    "        \n",
    "        # Calculate Gradient using the chain rule\n",
    "        # d(error)/d(z) = d(error)/d(pred) * d(pred)/d(z) -> where z is the input to the activation function (weight*x + b)\n",
    "        self.output_error = self.error_estimation_derv(y_pred, y_true) * self.activation_derv(y_pred,self.output_act_fn)\n",
    "\n",
    "        # Update output layer Weight and bias\n",
    "        self.output_bias -= learning_rate * np.sum(self.output_error,axis=0)\n",
    "        self.hidden_output_weights -= learning_rate * (self.hidden.T @ self.output_error) #(self.hidden.T @ self.output) How each hidden neuron should change\n",
    "        #(self.hidden.T @ self.output) -> d(error)/wij then multiply it by learning rate, .T to make it work (Incompatible sizes) \n",
    "\n",
    "        # Update hidden layer weights and biases\n",
    "        self. hidden_error = (self.output_error @ self.hidden_output_weights.T)  * self.activation_derv(self.hidden,'relu')# *self.activation_derv(self.hidden,'relu') because we are backpropagating through the activation function\n",
    "        self.n_hidden_biases -= learning_rate*np.sum(self.hidden_error,axis=0)\n",
    "        self.input_hidden_weights -= learning_rate*(X.T @ self.hidden_error) # d(error)/d(input_hidden_weights)\n",
    "        #(self.output_error @ self.hidden_output_weights.T) means how much did the hidden neuron contributed to the output error (Scale of each weight to be updated)\n",
    "        #d(error)/d(hidden_neuron_weights)\n",
    "\n",
    "    def train(self,X,y,learning_rate,epochs):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward_pass(X)\n",
    "            self.backpropagation(X,y,y_pred,learning_rate)\n",
    "            if epoch%1000 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {self.error_estimation(y,y_pred)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.error_fn == 'mse':\n",
    "            return self.forward_pass(X)\n",
    "        elif self.error_fn == 'log loss':\n",
    "            return np.where(self.forward_pass(X)>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6012270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=5000, n_features=5, noise=50, random_state=42)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81fa832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 26410.43126798775\n",
      "Epoch: 1000, Loss: 2342.38964105498\n",
      "Epoch: 2000, Loss: 2304.659287732356\n",
      "Epoch: 3000, Loss: 2278.4600310271503\n",
      "Epoch: 4000, Loss: 2255.8950609560025\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(X_train,y_train,128)\n",
    "nn.train(X_train, y_train,learning_rate=0.001,epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cde98533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8808559784087395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "print(r2_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddcaeff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8837168363777849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(r2_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40758fb8",
   "metadata": {},
   "source": [
    "- The Hand made NN explains 88.08% of the variance in the data while LinearRegression model explains 88.37% of the data. Pretty close ratio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
